{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benjaminward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For retrieving the clauses and labels.\n",
    "import os\n",
    "import json\n",
    "# For the duplicates\n",
    "from collections import defaultdict, Counter\n",
    "# For BERT\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# For deduplication\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "nltk.download('punkt')\n",
    "# For metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# For plots \n",
    "import matplotlib.pyplot as plt\n",
    "# For hyperparameter tuning\n",
    "from itertools import product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the individual clauses and labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n",
      "\n",
      "‚úÖ Extracted 14407 clause-rating pairs from the first two companies.\n",
      "\n",
      "('Instead of asking directly, this Service will assume your consent merely from your usage.', 'bad')\n",
      "('This service tracks which web page referred you to it', 'bad')\n",
      "('The service can sell or otherwise transfer your personal data as part of a bankruptcy proceeding or other type of financial transaction.', 'bad')\n",
      "('You must provide your legal name, pseudonyms are not allowed', 'bad')\n",
      "('This service employs third-party cookies, but with opt-out instructions', 'bad')\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"  # Data directory containing company folders\n",
    "clause_pairs = []\n",
    "\n",
    "# Step 1: Check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"‚ùå ERROR: Data directory '{data_dir}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Initialize a counter for the companies\n",
    "company_counter = 0\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    # Check if it's a directory (company folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        # Step 4: Check if clauses.json exists\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"‚ùå ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 5: Check if clauses.json is valid JSON\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Step 6: Check if 'clauses' key exists\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"‚ö†Ô∏è WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"‚ö†Ô∏è WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Step 7: Extract (title, rating) pairs\n",
    "            for clause in clauses:\n",
    "                title = clause.get(\"title\", \"\").strip() if clause.get(\"title\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if title and rating:\n",
    "                    clause_pairs.append((title, rating))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è WARNING: Skipping a clause in '{company}' due to missing title or rating.\")\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERROR: Invalid JSON in '{company}/clauses.json'\")\n",
    "\n",
    "        # Step 8: Stop after the first 2 companies\n",
    "        company_counter += 1\n",
    "        if company_counter >= 900:\n",
    "            break  # Exit the loop after processing the first two companies\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n‚úÖ Extracted {len(clause_pairs)} clause-rating pairs from the first two companies.\\n\")\n",
    "for pair in clause_pairs[:5]:  # Print first 5 for checking\n",
    "    print(pair)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Duplicate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n"
     ]
    }
   ],
   "source": [
    "clause_pairs = []  # Reset to include folder info\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"‚ùå ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"‚ö†Ô∏è WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"‚ö†Ô∏è WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Store (title, rating, folder)\n",
    "            for clause in clauses:\n",
    "                title = clause.get(\"title\", \"\").strip() if clause.get(\"title\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if title and rating:\n",
    "                    clause_pairs.append((title, rating, company))  # Store the folder name\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è WARNING: Skipping a clause in '{company}' due to missing title or rating.\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERROR: Invalid JSON in '{company}/clauses.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 10 most duplicated clause-rating pairs:\n",
      "Clause: There is a date of the last update of the agreements\n",
      "Rating: neutral\n",
      "Occurrences: 198\n",
      "Found in folders: Nextcloud, Authy (Twilio), Replika, Elk.zone, BFM TV, EthanMcBloxxer, IVPN, Metager, Cisco, Bilibili, MyAnimeList, Arch Linux, Pinterest, DrugBank, Canvas, Minecraft, mastodon.social, Tumblr, iFunny, Forbes, Audacity, xda-developers, Booking.com, Gap Creek Media, Enpass, SpigotMC, Guiding Tech, FaceApp, Opera, iCloud, Jitsi, Le Monde, Disney+, How-To Geek, MobyGames, Among Us, Waterfox, Wish, Medium, Gettr, ProtonVPN, Headspace, Prisma Media, Fedora, Odysee, Crain's Chicago Business, Skillshare, Orange, VPN.AC, Xiaomi, Open Collective, Conjuguemos, Google Chrome, Free Software Foundation, Nslookup, Toggl Track, Le Parisien, KDE, MySudo, Free Code Camp, Fedora Email, Leetify, NordVPN, ClassDojo, Busuu, Alibaba, Symbaloo, BudgetBakers, kik-messenger, ePlus Technology, Getty Images, Zoho, eBuddy, GeeksforGeeks, Tubi, Mega, Zoom Video Communications, Dark Reader, The Filipino Channel, Tuta, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), F-List, Astronomer, JojoYou (PriEco), The Movie Database (TMDb), Gather Town, Cryptomator, Washington Post, Reddit, Credit Karma, Schoology, Instagram, Archive of Our Own, Samsung, Huawei, Roadtrip Nation, Flickr, Represent, ShortcutWorld.com, Privacy.com, YNAB. (You Need a Budget), Encyclopedia Britannica, MuseScore, Dropbox, Vivaldi, Brilliant, Netflix, Linguee, Weatheralex1 Hub, Microspot, Le Bon Coin, Guilded, GoDaddy, Cond√© Nast, SpanishDict, Brave, Disroot, Unity, FreeTube, GlobaliD, FairTec, Recipe Realm, Merriam-Webster, Apple Services, Avast, ptgms Industries, Weblate, OsmAnd, TechByte Net, Deveroonie.xyz, Selfie2Anime, Follow My Health, Notion, Malwarebytes, TourOfCalifornia.org, Marmiton, TubeBuddy, Twitch, ADT, hCaptcha, OpenStreetMap, Heaven HR, Swiss Government, Le Soir, Google, SpaceHey, Yello, The Walt Disney Company, WikiTree, CodeSandbox, Strava, STiBaRC, Douban, Wise, Speedtest by Ookla, BBVA, HideMyAss!, Eltiempo.es, Parler, LBRY, Pantip.com, Prezi, Vox Media, Simitless, Gfycat, Lichess, Mozilla Thunderbird, DeviantArt, FanFiction, RethinkDNS, APK Pure, Calendly, Future PLC, Translate, CNN, Uber, PayPal, Vercel, Consumer News & Business Channel, Jeuxvideo.com, Codeberg, CouchSurfing, Flipboard, Yahoo!, NBC News, Wallapop, Free\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You are responsible for maintaining the security of your account and for the activities on your account\n",
      "Rating: neutral\n",
      "Occurrences: 189\n",
      "Found in folders: Nextcloud, Authy (Twilio), Fox News, Replika, El Pa√≠s, OpenAI, Disqus, Cisco, Bilibili, MyAnimeList, Discovery, Team SDS, Medscape, Pinterest, DrugBank, Minecraft, WhatsApp, SpigotMC, Forbes, xda-developers, Icedrive, PureVPN, El Mundo (Spain), FaceApp, Search Encrypt, GitHub, Artspace, DiscourseHosting, Opera, Le Monde, Disney+, Ziff Davis, Remind, Gettr, Coinbase, InfinityFree, Filen, FileFactory, Spotify, UptimeRobot, Simply Earth, Fiverr, Open Collective, Brainly, likee, SourceForge, Fivestars, Le Parisien, Fandango, Free Code Camp, Fedora Email, Leetify, NordVPN, 23andMe, Busuu, Alibaba, Symbaloo, BudgetBakers, Doctolib, ASKfm, Redbubble, SourceHut, Pico, Genius, Getty Images, CollegeBoard, Canva, Waze, Stingle Photos, Tubi, Mega, Zoom Video Communications, Duolingo, The Filipino Channel, Atlassian, Xfinity, Bitwarden, Metacafe, Minds, F-List, Blizzard, Ouigo, Yleisradio, Wattpad, Vimeo, HP | Hewlett-Packard, Profilic, Quizlet, Foursquare, Alan Thomson Simulation, Instructure, VKontakte, Scratch, Kahoot!, FamilyTreeDNA, Turnitin, Washington Post, Reddit, Amazon AWS, Credit Karma, Collins Dictionary, Discogs, IMDb, Huawei, Roadtrip Nation, Letterboxd, Privacy.com, Doodle, Goodreads, matrix.org, WikiHow, Dendreo, Element, Brilliant, Netflix, WordReference, Guilded, McDonald's, Veepn, MeWe, Unity, Merriam-Webster, Signal, CNED, Selfie2Anime, AliExpress, webmail, Follow My Health, HuffPost, Malwarebytes, Smartmessages, Qwant, Twitch, BetterHelp, Pearltrees, hCaptcha, OpenStreetMap, Firefox Cloud Services, Google, Slack, The Walt Disney Company, ExpressVPN, Strava, Douban, Wise, Speedtest by Ookla, Observable, Pexgle, Dailymotion, Khan Academy, X, ViacomCBS, Prezi, Liberapay, Crunchyroll, Gfycat, YTMP3.CC, DeviantArt, Rofkin, Patook, Wolt, FanFiction, Yuka, Calendly, Chapril, SC Johnson, Future PLC, Translate, Riot Games (League of Legends), CNN, Kongregate, Burger King, PayPal, Vercel, New York Times, Tinder, Sync, Bandcamp, Mailfence, Codeberg, CouchSurfing, National Geographic, Porton Private Cloudstorage, Wishlist, Wallapop\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service is provided 'as is' and to be used at the users' sole risk\n",
      "Rating: neutral\n",
      "Occurrences: 188\n",
      "Found in folders: Authy (Twilio), IVPN, Wikimedia, Disqus, Cisco, Bilibili, Medscape, Discovery, Pinterest, DrugBank, Minecraft, Steam, Tumblr, WhatsApp, iFunny, Forbes, xda-developers, Quora, Open Humans, Bitly, Enpass, LightyearVPN, Guiding Tech, XE.com, Alzforum, Search Encrypt, FastMail, Microsoft Services, GitHub, Artspace, Jitsi, Le Monde, ISODME, LinkedIn, MobyGames, ProtonVPN, Coinbase, F-Droid, Attendify, Crain's Chicago Business, BitChute, FileFactory, Surfshark, RiseUp.net, VPN.AC, Wikipedia, Simply Earth, Fiverr, Urban Dictionary, Brainly, likee, SourceForge, Fivestars, Le Parisien, MySudo, Free Code Camp, NordVPN, Fandango, Tripadvisor, BudgetBakers, ASKfm, CyberGhost, Hacker Wars, Genius, eBuddy, Instabridge, Waze, Mega, Zoom Video Communications, Signal Stickers, Pornhub, Xfinity, SparkNotes, Internet Archive, Bitwarden, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), Blizzard, Wattpad, Nulled, Vimeo, Quizlet, Foursquare, Yandex, Instructure, VKontakte, Scratch, Kahoot!, Turnitin, Washington Post, Reddit, Amazon AWS, Schoology, Instagram, Kaspersky, Archive of Our Own, Huawei, ShortcutWorld.com, Doodle, Goodreads, YouTube, Encyclopedia Britannica, WikiHow, Dendreo, Dropbox, Vivaldi, Element, United States Postal Service, Weatheralex1 Hub, Guilded, GoDaddy, McDonald's, Brave, Unity, Ting Inc., WeMod, BlueMail, Merriam-Webster, Apple Services, Avast, Rumble, Signal, CNED, webmail, Session, AirVPN, Follow My Health, HuffPost, Smartmessages, Presearch, Twitch, ADT, BetterHelp, hCaptcha, OpenStreetMap, Upcloud, Firefox Cloud Services, Ubisoft, W3Schools, KidzSearch, Slack, The Walt Disney Company, ExpressVPN, WikiTree, Strava, Observable, Pexgle, Virtual World Computing Cocoon MyData Rewards, Dailymotion, Khan Academy, HideMyAss!, Heroku, Patreon, Shopify, X, Meredith, Ryver, Parler, LBRY, Prezi, Vox Media, pCloud, Adobe Services, Gfycat, YTMP3.CC, DeviantArt, Patook, FanFiction, Healthline, Chapril, Future PLC, Riot Games (League of Legends), CNN, Kongregate, Amazon, New York Times, Consumer News & Business Channel, OneSignal, AnonAddy, Sync, Bandcamp, eBay, edX, Mailfence, Yahoo!, Pure Dating, NBC News, Codeberg\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: This service assumes no liability for any losses or damages resulting from any matter relating to the service\n",
      "Rating: neutral\n",
      "Occurrences: 187\n",
      "Found in folders: Authy (Twilio), Listudy, Replika, OpenAI, IVPN, Open Source Initiative, Cisco, Medscape, Discovery, Pinterest, DrugBank, iFunny, Audacity, xda-developers, Booking.com, PureVPN, Enpass, Anonfiles, Guiding Tech, XE.com, FaceApp, Alzforum, Search Encrypt, GitHub, Artspace, DiscourseHosting, Opera, Jitsi, Wickr Me, Le Monde, Disney+, LinkedIn, MobyGames, Among Us, Remind, Wish, Gettr, Coinbase, Attendify, Odysee, Crain's Chicago Business, BitChute, FileFactory, RiseUp.net, WeTransfer, Fiverr, Urban Dictionary, likee, SourceForge, Google Analytics, Nslookup, Fivestars, Le Parisien, MySudo, NordVPN, Leetify, ClassDojo, Tripadvisor, Busuu, Alibaba, Symbaloo, ASKfm, Redbubble, SourceHut, Getty Images, eBuddy, Instabridge, Stingle Photos, Tubi, Mega, Zoom Video Communications, The Filipino Channel, Metacafe, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), Blizzard, Ouigo, Houseparty, Wattpad, JojoYou (PriEco), Nulled, Vimeo, HP | Hewlett-Packard, Profilic, Gather Town, Alan Thomson Simulation, Instructure, VKontakte, FamilyTreeDNA, Washington Post, Reddit, Instagram, Truth Social, IMDb, Huawei, Represent, ShortcutWorld.com, Index Education, Privacy.com, Goodreads, YNAB. (You Need a Budget), matrix.org, Encyclopedia Britannica, WikiHow, Dropbox, Vivaldi, Element, Brilliant, United States Postal Service, Cloudflare, Weatheralex1 Hub, Le Bon Coin, Guilded, GoDaddy, McDonald's, SpanishDict, Veepn, Brave, Ting Inc., WeMod, Merriam-Webster, Avast, Rumble, Weblate, Signal, webmail, Session, AirVPN, HuffPost, Malwarebytes, TourOfCalifornia.org, Qwant, Presearch, ADT, hCaptcha, OpenStreetMap, Google, USSeek, W3Schools, KidzSearch, The Walt Disney Company, ExpressVPN, CodeSandbox, Strava, Meower, Speedtest by Ookla, Observable, Pexgle, Dailymotion, Khan Academy, HideMyAss!, Heroku, Patreon, Shopify, X, YouGov, t2bot.io, Meredith, Ryver, Parler, LBRY, Prezi, Tickets Plus, Adobe Services, Crunchyroll, Gfycat, YTMP3.CC, Creative Commons, DeviantArt, Rofkin, Patook, Healthline, Tresorit, APK Pure, Calendly, Future PLC, Translate, Riot Games (League of Legends), Blooket, AnonAddy, Jeuxvideo.com, Sync, eBay, edX, Mailfence, Yahoo!, Flipboard, Pure Dating\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service\n",
      "Rating: neutral\n",
      "Occurrences: 178\n",
      "Found in folders: Listudy, El Pa√≠s, OpenAI, IVPN, Wikimedia, Disqus, Open Source Initiative, MyAnimeList, Bilibili, Discovery, DrugBank, Minecraft, Steam, Tumblr, WhatsApp, iFunny, xda-developers, BBC, Icedrive, PureVPN, Enpass, El Mundo (Spain), Anonfiles, Guiding Tech, FaceApp, Microsoft Services, GitHub, Artspace, Jitsi, Wickr Me, Le Monde, LinkedIn, MobyGames, Among Us, Marca, Wish, Ableton, Gettr, Coinbase, Headspace, InfinityFree, BitChute, FileFactory, Skillshare, Surfshark, RiseUp.net, WeTransfer, Wikipedia, Simply Earth, V.H. Hess, Urban Dictionary, Brainly, SourceForge, Fivestars, NordVPN, 23andMe, ClassDojo, Viagogo, Alibaba, CyberGhost, Hacker Wars, Redbubble, eBuddy, Waze, Stingle Photos, Tubi, Mega, Zoom Video Communications, Signal Stickers, The Filipino Channel, Xfinity, SparkNotes, Bitwarden, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), Adventurer's Codex, F-List, Blizzard, Vimeo, HP | Hewlett-Packard, Yandex, Scratch, Washington Post, Reddit, Schoology, Instagram, Archive of Our Own, Truth Social, Discogs, Represent, ShortcutWorld.com, Privacy.com, Doodle, YouTube, YNAB. (You Need a Budget), matrix.org, WikiHow, Vivaldi, Element, Brilliant, United States Postal Service, Le Bon Coin, Guilded, McDonald's, Veepn, Bumble, Unity, Ting Inc., WeMod, Apple Services, Avast, Rumble, OsmAnd, Signal, CNED, webmail, Session, AirVPN, HuffPost, Synonymo, TourOfCalifornia.org, Smartmessages, Qwant, Presearch, BetterHelp, Pearltrees, hCaptcha, OpenStreetMap, Upcloud, Ubisoft, W3Schools, TikTok, ExpressVPN, Vouchley, CodeSandbox, Strava, Meower, Douban, Speedtest by Ookla, Observable, Pexgle, Virtual World Computing Cocoon MyData Rewards, Dailymotion, Khan Academy, HideMyAss!, Shopify, X, t2bot.io, Eltiempo.es, Ryver, LBRY, Le Figaro, Prezi, Vox Media, pCloud, Simitless, Liberapay, Crunchyroll, Gfycat, DeviantArt, MIT App Inventor, Wolt, Healthline, Future PLC, Translate, CNN, Uber, Burger King, Vercel, New York Times, Consumer News & Business Channel, Sync, edX, National Geographic, Pure Dating, Yahoo!, Porton Private Cloudstorage\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You can opt out of promotional communications\n",
      "Rating: good\n",
      "Occurrences: 169\n",
      "Found in folders: Nextcloud, Authy (Twilio), Disqus, Open Source Initiative, Hacking with Swift, Cisco, Discovery, Pinterest, Steam, Tumblr, iFunny, xda-developers, BBC, Quora, Booking.com, PureVPN, Enpass, Guiding Tech, XE.com, FaceApp, Alzforum, Amnesty International Limited (UK), Search Encrypt, FastMail, Artspace, Opera, Le Monde, Disney+, ISODME, Ziff Davis, MobyGames, Swisscows, Wish, Medium, Startpage, Gettr, ProtonVPN, Coinbase, Headspace, Attendify, LibreOffice, Stack Overflow, Odysee, Crain's Chicago Business, FileFactory, UptimeRobot, Xiaomi, Simply Earth, Fiverr, Open Collective, Standard Notes, SourceForge, Free Software Foundation, Fivestars, MySudo, NordVPN, 23andMe, Symbaloo, kik-messenger, Redbubble, SourceHut, Pico, Zoho, Genius, CollegeBoard, GeeksforGeeks, Canva, Stingle Photos, Tubi, Mega, Zoom Video Communications, Duolingo, The Filipino Channel, Atlassian, Mozilla.org, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), Adventurer's Codex, Yleisradio, Wattpad, HP | Hewlett-Packard, Profilic, Foursquare, WordPress.org, Alan Thomson Simulation, Instructure, Kahoot!, Pocket, Washington Post, Reddit, IKEA, BlaBlaCar, El Mundo (El Salvador), Collins Dictionary, BitDefender, Kaspersky, Truth Social, Huawei, Represent, Privacy.com, Goodreads, Encyclopedia Britannica, WikiHow, MuseScore, Dropbox, Vivaldi, Brilliant, Netflix, United States Postal Service, Microspot, GoDaddy, Cond√© Nast, SpanishDict, Unity, Ting Inc., WeMod, BlueMail, Merriam-Webster, Clockify, Rumble, Weblate, Malwarebytes, Qwant, Twitch, Amazon Prime Video, SmartNews, Heaven HR, Le Soir, Firefox Cloud Services, The Walt Disney Company, ExpressVPN, CodeSandbox, Speedtest by Ookla, Pexgle, Khan Academy, BBVA, L'Equipe, HideMyAss!, X, Meredith, ViacomCBS, Eltiempo.es, Parler, Moodle, LBRY, Le Figaro, Prezi, privacypolicies.com, Adobe Services, Crunchyroll, Gfycat, Creative Commons, DeviantArt, Wolt, Healthline, Calendly, Future PLC, Riot Games (League of Legends), Amazon, PayPal, Vercel, OneSignal, Jeuxvideo.com, Bandcamp, eBay, CouchSurfing, Flipboard, KeyMe\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You have the right to leave this service at any time\n",
      "Rating: good\n",
      "Occurrences: 169\n",
      "Found in folders: Nextcloud, Authy (Twilio), Listudy, Replika, Bereal, OpenAI, Snapchat, Minecraft, mastodon.social, Tumblr, Facebook, Hulu, Audacity, BBC, Open Humans, LightyearVPN, PureVPN, ToS;DR Phoenix, FastMail, GitHub, DiscourseHosting, Opera, iCloud, Jitsi, Wickr Me, Le Monde, Disney+, LinkedIn, Remind, Wish, Ableton, Medium, Gettr, Coinbase, LibreOffice, Stack Overflow, Odysee, BitChute, RiseUp.net, Wikipedia, V.H. Hess, likee, Conjuguemos, Standard Notes, Google Analytics, Google Chrome, Nslookup, MySudo, Free Code Camp, Fedora Email, NordVPN, 23andMe, ClassDojo, Tripadvisor, Busuu, Brax.Me, ASKfm, CyberGhost, Redbubble, eBuddy, Instabridge, Waze, Stingle Photos, Roblox, Tubi, Zoom Video Communications, Mega, Xfinity, Bitwarden, Metacafe, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), Houseparty, Astronomer, Wattpad, Vimeo, HP | Hewlett-Packard, Quizlet, Foursquare, Crowdmark, Instructure, FamilyTreeDNA, Reddit, Instagram, Kaspersky, Samsung, SpiderOak, Discogs, zmudzinski.me Personal Blog (DEPRECATED), Represent, Privacy.com, YouTube, Privacy Guides, Dendreo, The Session, Dropbox, Brilliant, Vivaldi, Netflix, United States Postal Service, Guilded, GoDaddy, McDonald's, SpanishDict, Brave, Bumble, Unity, MeWe, Ting Inc., Clockify, Apple Services, Line Corporation, Signal, Follow My Health, HuffPost, Malwarebytes, Smartmessages, Presearch, Twitch, Pearltrees, hCaptcha, OpenStreetMap, Diario AS, Upcloud, Firefox Cloud Services, Ubisoft, Google, Yello, Slack, The Walt Disney Company, WikiTree, CodeSandbox, Strava, Meower, Observable, Pexgle, Virtual World Computing Cocoon MyData Rewards, Khan Academy, Heroku, Patreon, X, YouGov, Parler, LBRY, Liberapay, pCloud, Adobe Services, Crunchyroll, Rofkin, Patook, Wolt, Yuka, Healthline, Tresorit, Pixel8Earth, Chapril, Kongregate, PayPal, Vercel, New York Times, Consumer News & Business Channel, OneSignal, AnonAddy, Sync, Bandcamp, edX, Mailfence, Codeberg, Porton Private Cloudstorage, Prisma Media\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: This service gives your personal data to third parties involved in its operation\n",
      "Rating: bad\n",
      "Occurrences: 161\n",
      "Found in folders: Nextcloud, Authy (Twilio), Bereal, OpenAI, BFM TV, 10minutemail, Open Source Initiative, Discovery, Minecraft, mastodon.social, Tumblr, Hulu, iFunny, xda-developers, M√©t√©o France, Booking.com, PureVPN, FaceApp, Alzforum, Microsoft Services, Opera, iCloud, Le Monde, Disney+, LinkedIn, Ziff Davis, MobyGames, Among Us, Remind, Wish, Neurococi, Coinbase, Prisma Media, InfinityFree, Odysee, Crain's Chicago Business, FileFactory, Orange, UptimeRobot, Conjuguemos, Google Analytics, Google Chrome, Free Software Foundation, Le Parisien, Free Code Camp, Leetify, NordVPN, ClassDojo, Alibaba, Symbaloo, BudgetBakers, CyberGhost, Redbubble, ePlus Technology, Pico, Getty Images, Instabridge, GeeksforGeeks, Roblox, Zoom Video Communications, The Filipino Channel, Bitwarden, –û–¥–Ω–æ–∫–ª–∞ÃÅ—Å—Å–Ω–∏–∫–∏ (Ok.ru), F-List, Ouigo, Astronomer, Wattpad, HP | Hewlett-Packard, Profilic, Gather Town, Yandex, FamilyTreeDNA, Washington Post, Amazon AWS, Movistar, IKEA, Credit Karma, Schoology, BlaBlaCar, Collins Dictionary, Kaspersky, Samsung, IMDb, Roadtrip Nation, Represent, Index Education, Privacy.com, matrix.org, Encyclopedia Britannica, MuseScore, Element, Brilliant, United States Postal Service, Le Bon Coin, Guilded, GoDaddy, Cond√© Nast, SpanishDict, Veepn, Unity, Merriam-Webster, BlueMail, Clockify, Rumble, Realestate.com.au, Weblate, Line Corporation, CNED, Selfie2Anime, Notion, Malwarebytes, Marmiton, TubeBuddy, ADT, Pearltrees, Heaven HR, Bing, Le Soir, TikTok, Nvidia, Yello, The Walt Disney Company, ExpressVPN, Linksys, Speedtest by Ookla, Pexgle, Dailymotion, Walmart, BBVA, L'Equipe, HideMyAss!, Shopify, YouGov, Meredith, ViacomCBS, Eltiempo.es, Pantip.com, Le Figaro, privacypolicies.com, Vox Media, Crunchyroll, Gfycat, Rofkin, MIT App Inventor, Wolt, Calendly, SC Johnson, Future PLC, Gandi, Translate, CNN, Uber, OneSignal, Tinder, VPN Hamster, Jeuxvideo.com, NBC News, Flipboard, Wallapop, Free, Flatmates\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: Blocking first party cookies may limit your ability to use the service\n",
      "Rating: neutral\n",
      "Occurrences: 153\n",
      "Found in folders: Authy (Twilio), OpenAI, 10minutemail, Snapchat, Disqus, MyAnimeList, Bilibili, Medscape, Arch Linux, DrugBank, Steam, WhatsApp, Facebook, SpigotMC, xda-developers, M√©t√©o France, Hulu, Gap Creek Media, El Mundo (Spain), XE.com, FaceApp, GitHub, Artspace, iCloud, Le Monde, Disney+, LinkedIn, How-To Geek, Ziff Davis, Among Us, Waterfox, Marca, Wish, Ableton, Medium, Gettr, Neurococi, Headspace, LibreOffice, Fedora, Crain's Chicago Business, Orange, Urban Dictionary, likee, Toggl Track, Fivestars, Le Parisien, Leetify, Tripadvisor, Viagogo, Alibaba, Doctolib, Redbubble, ePlus Technology, Zoho, Instabridge, Canva, GeeksforGeeks, Roblox, Tubi, Mega, The Filipino Channel, Blizzard, Ouigo, JojoYou (PriEco), HP | Hewlett-Packard, OpenHAB, The Movie Database (TMDb), Foursquare, Alan Thomson Simulation, Instructure, Washington Post, Reddit, Amazon AWS, Credit Karma, Instagram, Kaspersky, El Mundo (El Salvador), BitDefender, Samsung, Flickr, Index Education, Privacy.com, Encyclopedia Britannica, Dropbox, Vivaldi, Brilliant, Netflix, United States Postal Service, Linguee, Guilded, McDonald's, Unity, WeMod, Merriam-Webster, Avast, Realestate.com.au, Rumble, CNED, AliExpress, Malwarebytes, Telegram, Qwant, TubeBuddy, Amazon Prime Video, Pearltrees, hCaptcha, Heaven HR, Swiss Government, Le Soir, Firefox Cloud Services, TikTok, Yello, ExpressVPN, Campus Innovations Kultur, Vouchley, Hinterland Games, Speedtest by Ookla, Khan Academy, BBVA, L'Equipe, HideMyAss!, Patreon, Shopify, X, Eltiempo.es, Parler, LBRY, Le Figaro, Prezi, privacypolicies.com, pCloud, Gfycat, Creative Commons, Wolt, FanFiction, Healthline, APK Pure, Future PLC, Translate, Riot Games (League of Legends), PayPal, Consumer News & Business Channel, VPN Hamster, Sync, edX, NBC News, Pure Dating, Free, Flatmates\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service does not guarantee accuracy or reliability of the information provided\n",
      "Rating: neutral\n",
      "Occurrences: 146\n",
      "Found in folders: El Pa√≠s, Wikimedia, Disqus, Open Source Initiative, Cisco, Bilibili, MyAnimeList, Discovery, Medscape, DrugBank, Canvas, Minecraft, WhatsApp, iFunny, Audacity, xda-developers, BBC, Quora, Bitly, Enpass, El Mundo (Spain), Guiding Tech, XE.com, Alzforum, Search Encrypt, Microsoft Services, GitHub, Artspace, Opera, Le Monde, Cakey Bot, MobyGames, Marca, Wish, ProtonVPN, Coinbase, Headspace, InfinityFree, Odysee, Crain's Chicago Business, BitChute, FileFactory, Surfshark, Fiverr, V.H. Hess, Brainly, likee, SourceForge, Nslookup, Fivestars, Fandango, NordVPN, ClassDojo, Tripadvisor, Busuu, Alibaba, ASKfm, Hacker Wars, Canva, Waze, Tubi, Mega, Zoom Video Communications, The Filipino Channel, Xfinity, SparkNotes, Internet Archive, Bitwarden, Metacafe, Minds, F-List, Wattpad, Nulled, HP | Hewlett-Packard, Return YouTube Dislike, Foursquare, FamilyTreeDNA, Washington Post, Reddit, Credit Karma, BlaBlaCar, Archive of Our Own, Huawei, Represent, ShortcutWorld.com, Privacy.com, YouTube, Privacy Guides, Vivaldi, United States Postal Service, Weatheralex1 Hub, McDonald's, Ting Inc., WeMod, Rumble, OsmAnd, Signal, Orchid VPN, webmail, HuffPost, Synonymo, TourOfCalifornia.org, The MOD Archive, Qwant, Presearch, Twitch, ADT, BetterHelp, OpenStreetMap, Swiss Government, Ubisoft, USSeek, W3Schools, TikTok, SpaceHey, KidzSearch, ExpressVPN, Strava, Hinterland Games, Douban, Observable, Virtual World Computing Cocoon MyData Rewards, Dailymotion, BBVA, Shopify, X, t2bot.io, Eltiempo.es, Ryver, LBRY, pCloud, Crunchyroll, Patook, Wolt, FanFiction, Riot Games (League of Legends), CNN, Burger King, Consumer News & Business Channel, eBay, edX, National Geographic, Pure Dating, Yahoo!, KeyMe\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each (clause, rating) pair\n",
    "pair_counts = Counter((title, rating) for title, rating, _ in clause_pairs)\n",
    "\n",
    "# Get the top 5 most duplicated pairs\n",
    "most_common_pairs = pair_counts.most_common(10)\n",
    "\n",
    "# Map each (clause, rating) to its folder locations\n",
    "pair_locations = defaultdict(set)\n",
    "for title, rating, folder in clause_pairs:\n",
    "    pair_locations[(title, rating)].add(folder)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nüîç Top 10 most duplicated clause-rating pairs:\")\n",
    "for (title, rating), count in most_common_pairs:\n",
    "    folders = list(pair_locations[(title, rating)])  # Convert set to list\n",
    "    print(f\"Clause: {title}\\nRating: {rating}\\nOccurrences: {count}\\nFound in folders: {', '.join(folders)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Processing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n",
      "\n",
      "‚úÖ Extracted 14407 clause-rating pairs from the first two companies.\n",
      "\n",
      "('Instead of asking directly, this Service will assume your consent merely from your usage.', 'bad')\n",
      "('This service tracks which web page referred you to it', 'bad')\n",
      "('The service can sell or otherwise transfer your personal data as part of a bankruptcy proceeding or other type of financial transaction.', 'bad')\n",
      "('You must provide your legal name, pseudonyms are not allowed', 'bad')\n",
      "('This service employs third-party cookies, but with opt-out instructions', 'bad')\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"  # Data directory containing company folders\n",
    "clause_pairs = []\n",
    "\n",
    "# Step 1: Check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"‚ùå ERROR: Data directory '{data_dir}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Initialize a counter for the companies\n",
    "company_counter = 0\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    # Check if it's a directory (company folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        # Step 4: Check if clauses.json exists\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"‚ùå ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 5: Check if clauses.json is valid JSON\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Step 6: Check if 'clauses' key exists\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"‚ö†Ô∏è WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"‚ö†Ô∏è WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Step 7: Extract (title, rating) pairs\n",
    "            for clause in clauses:\n",
    "                title = clause.get(\"title\", \"\").strip() if clause.get(\"title\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if title and rating:\n",
    "                    clause_pairs.append((title, rating))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è WARNING: Skipping a clause in '{company}' due to missing title or rating.\")\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERROR: Invalid JSON in '{company}/clauses.json'\")\n",
    "\n",
    "        # Step 8: Stop after the first 2 companies\n",
    "        company_counter += 1\n",
    "        if company_counter >= 900:\n",
    "            break  # Exit the loop after processing the first two companies\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n‚úÖ Extracted {len(clause_pairs)} clause-rating pairs from the first two companies.\\n\")\n",
    "for pair in clause_pairs[:5]:  # Print first 5 for checking\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal of duplicates: 14407 clause-rating pairs.\n",
      "‚úÖ Removed exact duplicates. 1123 unique clause-rating pairs.\n",
      "Number of clauses which appear more than once in our dataset: 447.\n",
      "Number of clauses which we removed because there were already present once: 13284\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize a set to store unique clause-rating pairs, and non-unique clause-rating pais.\n",
    "unique_clause_pairs = set()\n",
    "non_unique_clause_pairs = set()\n",
    "occurrences_of_non_unique_clause_pairs = 0\n",
    "\n",
    "# Step 2: Filter out duplicates by checking if the pair already exists in the set\n",
    "filtered_clause_pairs = []\n",
    "\n",
    "for title, rating in clause_pairs:\n",
    "    # Use a tuple of (description, rating) as the set key\n",
    "    pair = (title, rating)\n",
    "    \n",
    "    # If the pair is not in the set, add it to the filtered list and the set\n",
    "    if pair not in unique_clause_pairs:\n",
    "        filtered_clause_pairs.append(pair)\n",
    "        unique_clause_pairs.add(pair)\n",
    "    else:\n",
    "        non_unique_clause_pairs.add(pair)\n",
    "        occurrences_of_non_unique_clause_pairs +=1\n",
    "\n",
    "unique_clause_pairs = list(unique_clause_pairs)\n",
    "non_unique_clause_pairs = list(non_unique_clause_pairs)\n",
    "\n",
    "# Step 3: Check how many unique pairs there are\n",
    "print(f\"Before removal of duplicates: {len(clause_pairs)} clause-rating pairs.\")\n",
    "print(f\"‚úÖ Removed exact duplicates. {len(filtered_clause_pairs)} unique clause-rating pairs.\")\n",
    "print(f\"Number of clauses which appear more than once in our dataset: {len(non_unique_clause_pairs)}.\")\n",
    "print(f\"Number of clauses which we removed because there were already present once: {occurrences_of_non_unique_clause_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The court of law governing the terms depends on where the user is located\n",
      "Rating: neutral\n",
      "\n",
      "Title: The service is only available to United States residents and businesses\n",
      "Rating: bad\n",
      "\n",
      "Title: Your identity is used in ads that are shown to other users\n",
      "Rating: unknown\n",
      "\n",
      "Title: Terms may be changed any time at their discretion, without notice to you\n",
      "Rating: bad\n",
      "\n",
      "Title: Your private content may be accessed by people working for the service\n",
      "Rating: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of accessing the descriptions and ratings\n",
    "for pair in unique_clause_pairs[:5]:  # Print first 5 pairs for checking\n",
    "    title = pair[0]  # Clause description (x)\n",
    "    rating = pair[1]       # Clause rating (y)\n",
    "    print(f\"Title: {title}\\nRating: {rating}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BERT model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Filtering: removing pairs with \"unknown\" as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size: 1074\n",
      "Train+dev set size: 859\n",
      "Test set size: 215\n"
     ]
    }
   ],
   "source": [
    "# Step 3.1: Filter out clauses with 'unknown' ratings\n",
    "filtered_clause_pairs = [(title, rating) for title, rating in unique_clause_pairs if rating != \"unknown\"]\n",
    "\n",
    "# Split the filtered data\n",
    "clauses, ratings = zip(*filtered_clause_pairs)  # Extract clauses and their ratings\n",
    "\n",
    "# Map ratings to integers\n",
    "rating_dict = {\"very bad\": 0, \"bad\": 1, \"neutral\": 2, \"good\": 3}  # Modify if you have different ratings\n",
    "ratings_int = [rating_dict[r] for r in ratings]\n",
    "\n",
    "# Step 3.2: Split data into train, dev, and test sets (80% train, 10% dev, 10% test)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(clauses, ratings_int, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print size\n",
    "print(f\"Data set size: {len(filtered_clause_pairs)}\")\n",
    "print(f\"Train+dev set size: {len(y_temp)}\")\n",
    "print(f\"Test set size: {len(y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Deduplication based on n-gram similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=3):\n",
    "    \"\"\"Convert text into a set of n-grams.\"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in ENGLISH_STOP_WORDS]  # Remove stopwords and non-alphanumeric\n",
    "    return set(ngrams(tokens, n))\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def deduplicate_test_set(train_clauses, test_clauses, test_labels, threshold=0.7, n=3):\n",
    "    \"\"\"Remove test clauses that are too similar to any train clause based on n-gram Jaccard similarity.\"\"\"\n",
    "    train_ngrams = [get_ngrams(clause, n) for clause in train_clauses]\n",
    "    \n",
    "    filtered_test_clauses = []\n",
    "    filtered_test_labels = []\n",
    "    \n",
    "    for test_clause, test_label in zip(test_clauses, test_labels):\n",
    "        test_ngram_set = get_ngrams(test_clause, n)\n",
    "        \n",
    "        # Check similarity with each train clause\n",
    "        max_similarity = max(jaccard_similarity(test_ngram_set, train_set) for train_set in train_ngrams)\n",
    "        \n",
    "        if max_similarity < threshold:\n",
    "            filtered_test_clauses.append(test_clause)\n",
    "            filtered_test_labels.append(test_label)\n",
    "    \n",
    "    return filtered_test_clauses, filtered_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication: test+dev set size is 215\n",
      "After deduplication: test+dev set size is 158\n"
     ]
    }
   ],
   "source": [
    "# Apply deduplication\n",
    "print(f\"Before deduplication: test+dev set size is {len(y_test)}\")\n",
    "X_test, y_test = deduplicate_test_set(X_temp, X_test, y_test, threshold=0.7, n=3)\n",
    "print(f\"After deduplication: test+dev set size is {len(y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Preparing for training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 730\n",
      "Dev set size: 129\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
    "print(f\"Train set size: {len(y_train)}\")\n",
    "print(f\"Dev set size: {len(y_dev)}\")\n",
    "\n",
    "# Step 3.3: Convert into a format that Hugging Face can use\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "dev_data = Dataset.from_dict({\"text\": X_dev, \"label\": y_dev})\n",
    "test_data = Dataset.from_dict({\"text\": X_test, \"label\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16432) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04e923b2cfa4b4e96e603b1f06dc219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa78b00357ad4cfc944038fd506385c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b974be83f7646498c37f3ccf8d302c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e443c90eca421db9675673d30ec4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d447dac93f54e3eafaf9287410028e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3696fdf147a04305b13a11c1b135f8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfe3c2d7e594063bc25ef9370ea9168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_16380/3550178219.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92b1fe981004269a53ca6c47bd92464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_16380/3550178219.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03a1566abab4f2897893463a5660fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_16380/3550178219.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 4.2: Define a function to tokenize the input texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples.get('text', \"\"), padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Step 4.3: Apply the tokenizer to the train, dev, and test datasets\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "dev_data = dev_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 4.4: Set the format for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "dev_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "\n",
    "# Step 4.5: Remove the original text filed\n",
    "train_data = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "dev_data = dev_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "test_data = test_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Step 4.6: Make sure we are working with longs\n",
    "train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Take a smaller sample (e.g., 5%) of the training data\n",
    "train_sample = train_data.shuffle(seed=42).select(range(int(0.05 * len(train_data))))\n",
    "dev_sample = dev_data.shuffle(seed=42).select(range(int(0.1*len(dev_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)  # Convert logits to predicted labels\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Hyperparameter tuning for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=3e-05, epochs=3, dropout=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.527300</td>\n",
       "      <td>1.084982</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.133000</td>\n",
       "      <td>0.964292</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=3e-05, epochs=3, dropout=0.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 02:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.264800</td>\n",
       "      <td>1.099147</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.340278</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.429825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.185400</td>\n",
       "      <td>1.040638</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.340278</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.429825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=5e-05, epochs=3, dropout=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/54 00:35 < 01:13, 0.48 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/6 00:03 < 00:00, 1.09 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=5e-05, epochs=3, dropout=0.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/54 00:55 < 01:53, 0.31 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/6 00:00 < 00:00, 4.48 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: (2, 5e-05, 3, 0.1)\n",
      "Best Weighted F1 Score: 0.705128205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"batch_size\": [2],  # Try different batch sizes - use [4,8,16,32]\n",
    "    \"learning_rate\": [3e-5, 5e-5],  # Common BERT learning rates - use [2e-5,3e-5,5e-5]\n",
    "    \"num_epochs\": [3],  # Vary number of epochs - use [4,8,12,16]\n",
    "    \"dropout_rate\": [0.1, 0.2]  # Try different dropout rates - use [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Step 2: Track best model\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Step 3: Iterate over all hyperparameter combinations\n",
    "for batch_size, lr, epochs, dropout in product(param_grid[\"batch_size\"], \n",
    "                                               param_grid[\"learning_rate\"], \n",
    "                                               param_grid[\"num_epochs\"], \n",
    "                                               param_grid[\"dropout_rate\"]):\n",
    "    \n",
    "    print(f\"\\nTraining with batch_size={batch_size}, lr={lr}, epochs={epochs}, dropout={dropout}\\n\")\n",
    "\n",
    "    # Step 4: Modify model with dropout\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "    model.config.hidden_dropout_prob = dropout\n",
    "    model.config.attention_probs_dropout_prob = dropout\n",
    "    \n",
    "    # Step 5: Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",  # Don't save all models to save space\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # Step 6: Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_sample,\n",
    "        eval_dataset=dev_sample,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Step 7: Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Step 8: Get weighted F1-score\n",
    "    f1 = metrics.get(\"eval_f1\", 0)\n",
    "\n",
    "    # Step 9: Track best model\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = (batch_size, lr, epochs, dropout)\n",
    "        best_model = model  # Save the best model in memory\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Weighted F1 Score:\", best_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
