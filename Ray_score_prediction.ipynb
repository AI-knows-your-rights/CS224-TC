{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the individual clauses and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Function\n",
    "def load_clauses_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load clauses data from the specified directory\n",
    "    Parameters:\n",
    "        data_dir (str): Path to the directory containing service folders\n",
    "    Returns:\n",
    "        list: List of dictionaries containing clause data\n",
    "    \"\"\"\n",
    "    all_clauses = []\n",
    "    \n",
    "    for service_folder in os.listdir(data_dir):\n",
    "        service_path = os.path.join(data_dir, service_folder)\n",
    "        \n",
    "        if not os.path.isdir(service_path):\n",
    "            continue\n",
    "            \n",
    "        clauses_file = os.path.join(service_path, 'clauses.json')\n",
    "        if not os.path.exists(clauses_file):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(clauses_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            if 'clauses' not in data or not data['clauses']:\n",
    "                print(f\"‚ö†Ô∏è WARNING: 'clauses' list is empty in '{service_folder}/clauses.json'\")\n",
    "                continue\n",
    "                \n",
    "            for clause in data['clauses']:\n",
    "                if not all(key in clause for key in ['clause_text', 'description', 'rating']):\n",
    "                    print(f\"‚ö†Ô∏è WARNING: Skipping a clause in '{service_folder}' due to missing required fields\")\n",
    "                    continue\n",
    "                    \n",
    "                clause_data = {\n",
    "                    'service': service_folder,\n",
    "                    'clause_text': clause['clause_text'],\n",
    "                    'description': clause['description'],\n",
    "                    'rating': clause['rating']\n",
    "                }\n",
    "                all_clauses.append(clause_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {service_folder}: {str(e)}\")\n",
    "            \n",
    "    return all_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    'good': 1,\n",
    "    'bad': -1,\n",
    "    'blocker': -3,\n",
    "    'neutral': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n",
      "\n",
      "Rating distribution after conversion:\n",
      "rating\n",
      "-3     219\n",
      "-1    2896\n",
      " 0    4072\n",
      " 1    2606\n",
      "Name: count, dtype: int64\n",
      "Training set size: 7834\n",
      "Testing set size: 1959\n",
      "\n",
      "Rating distribution in training set:\n",
      "rating\n",
      "-3     178\n",
      "-1    2299\n",
      " 0    3263\n",
      " 1    2094\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "DATA_DIR = \"data_all_202503120623106\"\n",
    "clauses_data = load_clauses_data(DATA_DIR)\n",
    "df = pd.DataFrame(clauses_data)\n",
    "\n",
    "# Convert ratings to numerical values\n",
    "df['rating'] = df['rating'].map(rating_map)\n",
    "print(\"\\nRating distribution after conversion:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "# Split into training and testing sets\n",
    "\n",
    "training_size = round(df.shape[0] * 0.8)\n",
    "\n",
    "train_df, test_df = train_test_split(df, train_size=training_size, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")\n",
    "print(\"\\nRating distribution in training set:\")\n",
    "print(train_df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,  # Regression task\n",
    "    problem_type=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Convert the input to a list of strings and ensure it's properly formatted\n",
    "    texts = [str(text) for text in examples[\"clause_text\"]]  # Ensure text is string\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Important: keep this as None for batched processing\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "train_raw_data = prepare_dataset(train_df)\n",
    "test_raw_data = prepare_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['service', 'clause_text', 'description', 'rating', '__index_level_0__'],\n",
      "    num_rows: 7834\n",
      "})\n",
      "Dataset features: {'service': Value(dtype='string', id=None), 'clause_text': Value(dtype='string', id=None), 'description': Value(dtype='string', id=None), 'rating': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
      "Sample row: {'service': 'Vivaldi', 'clause_text': 'The Website and all the Website elements are provided on an ‚Äúas is‚Äù basis.', 'description': \"This service does not provide any guarantees as to its usability or fitness for the users' purposes. Users agree to use it at their own risk, accepting any possible bugs, malfunctions or harm to their devices.\", 'rating': 0, '__index_level_0__': 8183}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721bf61546354facb505cdb7673c00f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8479d4379a458094b0c0dffc5e614a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_raw_data)\n",
    "\n",
    "print(\"Dataset features:\", train_raw_data.features)\n",
    "print(\"Sample row:\", train_raw_data[0])\n",
    "\n",
    "\n",
    "train_dataset = train_raw_data.map(tokenize_function, batched=True)\n",
    "test_dataset = test_raw_data.map(tokenize_function, batched=True)\n",
    "\n",
    "columns_to_remove = ['service', 'clause_text', 'description']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
    "train_dataset = train_dataset.rename_column('rating', 'labels')\n",
    "test_dataset = test_dataset.rename_column('rating', 'labels')\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = ((predictions - labels) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse\n",
    "    }\n",
    "\n",
    "#BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayhu/miniconda3/envs/tc_ranker/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7834\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test set\n",
    "test_results = trainer.evaluate()\n",
    "print(\"\\nTest Results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Function to predict ratings for new clauses\n",
    "def predict_rating(clause_text):\n",
    "    inputs = tokenizer(\n",
    "        clause_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits.squeeze()\n",
    "    \n",
    "    return predictions.item()\n",
    "\n",
    "# Test prediction with a sample clause\n",
    "sample_clause = test_df['clause_text'].iloc[0]\n",
    "predicted_rating = predict_rating(sample_clause)\n",
    "actual_rating = test_df['rating'].iloc[0]\n",
    "\n",
    "print(\"\\nSample Prediction:\")\n",
    "print(f\"Predicted Rating: {predicted_rating:.2f}\")\n",
    "print(f\"Actual Rating: {actual_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_clause_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split the filtered data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m clauses, ratings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[43mfiltered_clause_pairs\u001b[49m)  \u001b[38;5;66;03m# Extract clauses and their ratings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Map ratings to integers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rating_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery bad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m}  \u001b[38;5;66;03m# Modify if you have different ratings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_clause_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the filtered data\n",
    "clauses, ratings = zip(*filtered_clause_pairs)  # Extract clauses and their ratings\n",
    "\n",
    "# Map ratings to integers\n",
    "rating_dict = {\"very bad\": 0, \"bad\": 1, \"neutral\": 2, \"good\": 3}  # Modify if you have different ratings\n",
    "ratings_int = [rating_dict[r] for r in ratings]\n",
    "\n",
    "# Step 3.2: Split data into train, dev, and test sets (80% train, 10% dev, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(clauses, ratings_int, test_size=0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 3.3: Convert into a format that Hugging Face can use\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "dev_data = Dataset.from_dict({\"text\": X_dev, \"label\": y_dev})\n",
    "test_data = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c690508f45427eaa076f6e00572826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f90c2736f4b47865602a1daaced8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46279b7d31f346edad8b5a7967e2afc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee84fa31270407f81d76d21da0ef6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809b6d292cfc4d40857e3b7f3b9bee79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5555646e7a43c1982e0fc09a448a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef043968df249eca16303c5ea116ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_15651/3550178219.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0018da2f39374962a258f9b1ca302e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_15651/3550178219.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4776289e9b624391b68b3650dc5aeeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_15651/3550178219.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 4.2: Define a function to tokenize the input texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples.get('text', \"\"), padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Step 4.3: Apply the tokenizer to the train, dev, and test datasets\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "dev_data = dev_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 4.4: Set the format for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "dev_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "\n",
    "# Step 4.5: Remove the original text filed\n",
    "train_data = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "dev_data = dev_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "test_data = test_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Step 4.6: Make sure we are working with longs\n",
    "train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Take a smaller sample (e.g., 5%) of the training data\n",
    "train_sample = train_data.shuffle(seed=42).select(range(int(0.1 * len(train_data))))\n",
    "dev_sample = dev_data.shuffle(seed=42).select(range(int(0.2 * len(dev_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",  # Update to eval_strategy\n",
    "    save_strategy=\"epoch\",  # Save model at each epoch\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)  # Convert logits to predicted labels\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='521' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 09:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>0.992183</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343900</td>\n",
       "      <td>1.054539</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>1.111654</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>1.603718</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.271020</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.280416</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.328581</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.335631</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.352474</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/6 00:00 < 00:00, 4.57 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=520, training_loss=0.22206129044330178, metrics={'train_runtime': 569.5669, 'train_samples_per_second': 1.808, 'train_steps_per_second': 0.913, 'total_flos': 271009253498880.0, 'train_loss': 0.22206129044330178, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    eval_dataset=dev_sample,\n",
    "    compute_metrics=compute_metrics  # Corrected function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8967\n",
      "Test Accuracy: 0.8538\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_data)\n",
    "\n",
    "# Print loss and accuracy\n",
    "print(f\"Test Loss: {test_results['eval_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")  # Accuracy from compute_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc_ranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
