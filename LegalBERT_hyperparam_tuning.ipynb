{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LegalBERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/benjaminward/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benjaminward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For retrieving the clauses and labels.\n",
    "import os\n",
    "import json\n",
    "# For the duplicates\n",
    "from collections import defaultdict, Counter\n",
    "# For BERT\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# For deduplication\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "nltk.download('punkt')\n",
    "# For metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# For plots \n",
    "import matplotlib.pyplot as plt\n",
    "# For hyperparameter tuning\n",
    "from itertools import product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the individual clauses and labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "âš ï¸ WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n",
      "\n",
      "âœ… Extracted 14407 clause-rating pairs from the first two companies.\n",
      "\n",
      "('Instead of asking directly, this Service will assume your consent merely from your usage.', 'bad')\n",
      "('This service tracks which web page referred you to it', 'bad')\n",
      "('The service can sell or otherwise transfer your personal data as part of a bankruptcy proceeding or other type of financial transaction.', 'bad')\n",
      "('You must provide your legal name, pseudonyms are not allowed', 'bad')\n",
      "('This service employs third-party cookies, but with opt-out instructions', 'bad')\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data_all_202503120623106\"  # Data directory containing company folders\n",
    "clause_pairs = []\n",
    "\n",
    "# Step 1: Check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"âŒ ERROR: Data directory '{data_dir}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Initialize a counter for the companies\n",
    "company_counter = 0\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    # Check if it's a directory (company folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        # Step 4: Check if clauses.json exists\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"âŒ ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 5: Check if clauses.json is valid JSON\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Step 6: Check if 'clauses' key exists\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"âš ï¸ WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"âš ï¸ WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Step 7: Extract (clause_text, rating) pairs\n",
    "            for clause in clauses:\n",
    "                clause_text = clause.get(\"clause_text\", \"\").strip() if clause.get(\"clause_text\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if clause_text and rating:\n",
    "                    clause_pairs.append((clause_text, rating))\n",
    "                else:\n",
    "                    print(f\"âš ï¸ WARNING: Skipping a clause in '{company}' due to missing clause_text or rating.\")\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ ERROR: Invalid JSON in '{company}/clauses.json'\")\n",
    "\n",
    "        # Step 8: Stop after the first 2 companies\n",
    "        company_counter += 1\n",
    "        if company_counter >= 900:\n",
    "            break  # Exit the loop after processing the first two companies\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nâœ… Extracted {len(clause_pairs)} clause-rating pairs from the first two companies.\\n\")\n",
    "for pair in clause_pairs[:5]:  # Print first 5 for checking\n",
    "    print(pair)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Duplicate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "âš ï¸ WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n"
     ]
    }
   ],
   "source": [
    "clause_pairs = []  # Reset to include folder info\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"âŒ ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"âš ï¸ WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"âš ï¸ WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Store (clause_text, rating, folder)\n",
    "            for clause in clauses:\n",
    "                clause_text = clause.get(\"clause_text\", \"\").strip() if clause.get(\"clause_text\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if clause_text and rating:\n",
    "                    clause_pairs.append((clause_text, rating, company))  # Store the folder name\n",
    "                else:\n",
    "                    print(f\"âš ï¸ WARNING: Skipping a clause in '{company}' due to missing clause_text or rating.\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ ERROR: Invalid JSON in '{company}/clauses.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top 10 most duplicated clause-rating pairs:\n",
      "Clause: There is a date of the last update of the agreements\n",
      "Rating: neutral\n",
      "Occurrences: 198\n",
      "Found in folders: Notion, Translate, Apple Services, Audacity, Guilded, Linguee, RethinkDNS, How-To Geek, Yello, HideMyAss!, IVPN, OpenStreetMap, Dark Reader, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), OsmAnd, LBRY, Free, EthanMcBloxxer, Getty Images, Credit Karma, Yahoo!, DrugBank, Symbaloo, CondÃ© Nast, xda-developers, JojoYou (PriEco), Wise, Gfycat, Replika, Fedora Email, Represent, Orange, F-List, Toggl Track, BFM TV, Simitless, Vivaldi, FanFiction, Weblate, Medium, Mozilla Thunderbird, Douban, Encyclopedia Britannica, TubeBuddy, NBC News, FairTec, Nslookup, Avast, kik-messenger, ePlus Technology, Bilibili, VPN.AC, Instagram, Unity, Speedtest by Ookla, Booking.com, WikiTree, Brilliant, YNAB. (You Need a Budget), Free Code Camp, Nextcloud, Consumer News & Business Channel, The Filipino Channel, MuseScore, Wallapop, The Walt Disney Company, Leetify, STiBaRC, Cryptomator, Reddit, Tumblr, Netflix, Flipboard, Conjuguemos, Malwarebytes, Jitsi, Prisma Media, Washington Post, KDE, TechByte Net, Follow My Health, Free Software Foundation, Disroot, Calendly, Among Us, Brave, Vercel, Le Monde, Mega, hCaptcha, GoDaddy, Twitch, The Movie Database (TMDb), Minecraft, NordVPN, Crain's Chicago Business, Wish, Eltiempo.es, ClassDojo, eBuddy, Pantip.com, PayPal, SpigotMC, Archive of Our Own, BBVA, Le Soir, Huawei, Waterfox, Flickr, Le Bon Coin, Metager, ptgms Industries, ProtonVPN, Merriam-Webster, Zoom Video Communications, Gettr, Fedora, CouchSurfing, Google Chrome, Gap Creek Media, Opera, Open Collective, Heaven HR, TourOfCalifornia.org, Headspace, Schoology, CodeSandbox, SpaceHey, MyAnimeList, Authy (Twilio), Canvas, Lichess, Vox Media, Selfie2Anime, Arch Linux, Forbes, Xiaomi, MySudo, ADT, Tubi, BudgetBakers, Parler, Disney+, APK Pure, iCloud, Enpass, MobyGames, Deveroonie.xyz, ShortcutWorld.com, Google, Microspot, Future PLC, Alibaba, Recipe Realm, Astronomer, Samsung, Marmiton, Pinterest, FaceApp, Le Parisien, DeviantArt, Gather Town, mastodon.social, Cisco, CNN, Prezi, Weatheralex1 Hub, Tuta, FreeTube, Codeberg, Odysee, Elk.zone, Guiding Tech, Uber, Roadtrip Nation, Swiss Government, Skillshare, GeeksforGeeks, Zoho, SpanishDict, Strava, Jeuxvideo.com, GlobaliD, iFunny, Busuu, Dropbox\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You are responsible for maintaining the security of your account and for the activities on your account\n",
      "Rating: neutral\n",
      "Occurrences: 189\n",
      "Found in folders: Yleisradio, Genius, Translate, Guilded, HP | Hewlett-Packard, OpenAI, Remind, OpenStreetMap, Privacy.com, Wolt, Blizzard, Getty Images, Burger King, Profilic, Credit Karma, DrugBank, Symbaloo, Kongregate, xda-developers, Wise, Gfycat, Team SDS, Replika, Fedora Email, X, Smartmessages, AliExpress, F-List, MeWe, Canva, FamilyTreeDNA, Pearltrees, Brainly, IMDb, FanFiction, CollegeBoard, Douban, Observable, Ouigo, Doodle, Firefox Cloud Services, FileFactory, Collins Dictionary, Medscape, Bilibili, YTMP3.CC, Fandango, WhatsApp, Unity, Foursquare, Speedtest by Ookla, Disqus, UptimeRobot, Brilliant, Free Code Camp, Nextcloud, Turnitin, The Filipino Channel, El PaÃ­s, Yuka, McDonald's, Letterboxd, likee, Wallapop, The Walt Disney Company, Leetify, ASKfm, WordReference, Artspace, Simply Earth, Dailymotion, Pexgle, Netflix, Reddit, Fiverr, Search Encrypt, Malwarebytes, Washington Post, Follow My Health, Calendly, Wishlist, Le Monde, Vercel, Element, Mega, Scratch, Alan Thomson Simulation, hCaptcha, Liberapay, Twitch, VKontakte, Goodreads, PureVPN, Metacafe, NordVPN, Minecraft, Coinbase, Quizlet, PayPal, Veepn, Redbubble, SC Johnson, SpigotMC, Sync, Chapril, Huawei, National Geographic, SourceForge, Zoom Video Communications, Gettr, Merriam-Webster, Wattpad, CouchSurfing, Open Collective, Opera, Vimeo, DiscourseHosting, Porton Private Cloudstorage, Pico, InfinityFree, MyAnimeList, Authy (Twilio), Selfie2Anime, Amazon AWS, Forbes, Waze, Tubi, BudgetBakers, Fivestars, Bandcamp, SourceHut, Qwant, Disney+, Ziff Davis, Xfinity, Doctolib, Mailfence, Discogs, WikiHow, Google, ExpressVPN, Fox News, CNED, Khan Academy, Kahoot!, Future PLC, ViacomCBS, Alibaba, Icedrive, Slack, Crunchyroll, Duolingo, Atlassian, Signal, Pinterest, FaceApp, Le Parisien, GitHub, DeviantArt, webmail, Cisco, BetterHelp, CNN, Prezi, HuffPost, Filen, Dendreo, Codeberg, Minds, Discovery, 23andMe, Instructure, Bitwarden, El Mundo (Spain), Roadtrip Nation, Rofkin, Patook, Riot Games (League of Legends), Tinder, Strava, New York Times, Busuu, Spotify, Stingle Photos, matrix.org\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service is provided 'as is' and to be used at the users' sole risk\n",
      "Rating: neutral\n",
      "Occurrences: 188\n",
      "Found in folders: Genius, Apple Services, Guilded, HideMyAss!, IVPN, OpenStreetMap, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), LinkedIn, eBay, Blizzard, LBRY, Alzforum, F-Droid, Yahoo!, DrugBank, Kongregate, xda-developers, Open Humans, Ubisoft, RiseUp.net, Gfycat, Smartmessages, X, Adobe Services, Amazon, Heroku, Attendify, Vivaldi, Kaspersky, Brainly, FanFiction, Encyclopedia Britannica, Pure Dating, FastMail, NBC News, Observable, Doodle, Firefox Cloud Services, FileFactory, Medscape, Avast, Bilibili, YTMP3.CC, VPN.AC, Nulled, Shopify, Instagram, Fandango, WhatsApp, Surfshark, Unity, Foursquare, Disqus, WikiTree, BlueMail, Quora, Free Code Camp, Turnitin, Session, Consumer News & Business Channel, Microsoft Services, McDonald's, likee, The Walt Disney Company, ASKfm, WeMod, Artspace, Simply Earth, Dailymotion, Pexgle, Tumblr, Reddit, Fiverr, Search Encrypt, Yandex, Jitsi, YouTube, CyberGhost, Wikipedia, Washington Post, Follow My Health, Pornhub, Brave, Element, Le Monde, Mega, Scratch, AnonAddy, hCaptcha, edX, GoDaddy, Twitch, VKontakte, Goodreads, Minecraft, NordVPN, Signal Stickers, Crain's Chicago Business, Steam, eBuddy, Coinbase, Quizlet, Archive of Our Own, Sync, Chapril, Huawei, ProtonVPN, SourceForge, Zoom Video Communications, Merriam-Webster, Wattpad, Upcloud, Vimeo, Wikimedia, Schoology, OneSignal, KidzSearch, BitChute, Authy (Twilio), SparkNotes, Vox Media, Amazon AWS, Forbes, Waze, MySudo, ADT, Rumble, LightyearVPN, BudgetBakers, Fivestars, Bandcamp, Ryver, Ting Inc., Parler, Healthline, Enpass, MobyGames, Xfinity, Mailfence, WikiHow, ShortcutWorld.com, ExpressVPN, Instabridge, CNED, Khan Academy, Kahoot!, Future PLC, Patreon, Hacker Wars, ISODME, Slack, Urban Dictionary, Signal, Pinterest, Le Parisien, Internet Archive, DeviantArt, GitHub, webmail, pCloud, W3Schools, Cisco, Virtual World Computing Cocoon MyData Rewards, BetterHelp, CNN, Bitly, Prezi, Weatheralex1 Hub, HuffPost, Codeberg, Dendreo, Discovery, Presearch, Instructure, Bitwarden, Guiding Tech, Meredith, United States Postal Service, Tripadvisor, Patook, Riot Games (League of Legends), Strava, New York Times, iFunny, AirVPN, XE.com, Dropbox\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: This service assumes no liability for any losses or damages resulting from any matter relating to the service\n",
      "Rating: neutral\n",
      "Occurrences: 187\n",
      "Found in folders: Tresorit, Translate, Audacity, Guilded, HideMyAss!, HP | Hewlett-Packard, OpenAI, Remind, OpenStreetMap, IVPN, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), LinkedIn, eBay, Blizzard, LBRY, Getty Images, Alzforum, Creative Commons, Profilic, Yahoo!, DrugBank, Symbaloo, xda-developers, JojoYou (PriEco), RiseUp.net, Gfycat, Replika, Index Education, X, Represent, Adobe Services, Heroku, Attendify, FamilyTreeDNA, Vivaldi, IMDb, Weblate, YouGov, Encyclopedia Britannica, Pure Dating, Tickets Plus, Ouigo, Observable, Nslookup, FileFactory, Medscape, Avast, YTMP3.CC, Nulled, Shopify, Instagram, Speedtest by Ookla, Booking.com, Brilliant, YNAB. (You Need a Budget), Session, The Filipino Channel, McDonald's, likee, The Walt Disney Company, Leetify, ASKfm, WeMod, WeTransfer, Artspace, Dailymotion, Pexgle, Reddit, Fiverr, Flipboard, Search Encrypt, Malwarebytes, Jitsi, Washington Post, Blooket, Calendly, Among Us, Brave, Element, Le Monde, Mega, AnonAddy, Alan Thomson Simulation, hCaptcha, edX, GoDaddy, VKontakte, Goodreads, PureVPN, Metacafe, NordVPN, Crain's Chicago Business, Wish, ClassDojo, eBuddy, Houseparty, Coinbase, Listudy, Veepn, Redbubble, Sync, Huawei, Le Bon Coin, SourceForge, Zoom Video Communications, Gettr, Merriam-Webster, Wattpad, Opera, Vimeo, TourOfCalifornia.org, DiscourseHosting, Wickr Me, CodeSandbox, Cloudflare, KidzSearch, BitChute, Authy (Twilio), MySudo, ADT, Tubi, Rumble, Fivestars, Open Source Initiative, Ryver, Ting Inc., SourceHut, Qwant, Parler, Disney+, Healthline, Google Analytics, APK Pure, Truth Social, Enpass, MobyGames, Mailfence, t2bot.io, USSeek, WikiHow, ShortcutWorld.com, Google, Instabridge, ExpressVPN, Khan Academy, Future PLC, Patreon, Alibaba, Anonfiles, Urban Dictionary, Crunchyroll, Signal, Pinterest, FaceApp, Le Parisien, GitHub, DeviantArt, Gather Town, webmail, W3Schools, Cisco, Prezi, Weatheralex1 Hub, HuffPost, Meower, Discovery, Odysee, Presearch, Instructure, Guiding Tech, Meredith, Rofkin, United States Postal Service, Tripadvisor, Patook, Riot Games (League of Legends), SpanishDict, Strava, Jeuxvideo.com, iFunny, Busuu, AirVPN, Stingle Photos, XE.com, Dropbox, matrix.org\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service\n",
      "Rating: neutral\n",
      "Occurrences: 178\n",
      "Found in folders: Translate, Apple Services, Guilded, HideMyAss!, HP | Hewlett-Packard, OpenAI, IVPN, OpenStreetMap, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), Wolt, OsmAnd, LinkedIn, Blizzard, LBRY, Burger King, Yahoo!, DrugBank, xda-developers, Ubisoft, RiseUp.net, Gfycat, Represent, Smartmessages, X, F-List, Simitless, Vivaldi, Pearltrees, Brainly, TikTok, Douban, Pure Dating, Synonymo, Observable, Doodle, FileFactory, Avast, Bilibili, Shopify, Vouchley, Instagram, WhatsApp, Surfshark, Unity, Speedtest by Ookla, Disqus, Brilliant, YNAB. (You Need a Budget), Session, Consumer News & Business Channel, Microsoft Services, The Filipino Channel, El PaÃ­s, McDonald's, Le Figaro, WeMod, WeTransfer, Artspace, Simply Earth, Dailymotion, Pexgle, Tumblr, Reddit, Viagogo, Yandex, Jitsi, YouTube, CyberGhost, Wikipedia, Washington Post, BBC, Among Us, Le Monde, Vercel, Element, Mega, Scratch, hCaptcha, edX, Liberapay, Minecraft, PureVPN, NordVPN, Signal Stickers, Wish, Eltiempo.es, Ableton, Steam, eBuddy, ClassDojo, Coinbase, Listudy, Veepn, Redbubble, Bumble, Archive of Our Own, Sync, Le Bon Coin, National Geographic, SourceForge, Zoom Video Communications, Gettr, Upcloud, Vimeo, TourOfCalifornia.org, Wickr Me, Wikimedia, Porton Private Cloudstorage, Headspace, Schoology, CodeSandbox, InfinityFree, MyAnimeList, BitChute, SparkNotes, Vox Media, Waze, Marca, Rumble, Tubi, Fivestars, Open Source Initiative, Ryver, Ting Inc., V.H. Hess, Qwant, Healthline, Truth Social, Enpass, MobyGames, Xfinity, t2bot.io, Discogs, WikiHow, ShortcutWorld.com, ExpressVPN, CNED, Khan Academy, Future PLC, Alibaba, Hacker Wars, Anonfiles, Icedrive, Urban Dictionary, Crunchyroll, Signal, FaceApp, GitHub, DeviantArt, webmail, pCloud, W3Schools, Virtual World Computing Cocoon MyData Rewards, BetterHelp, CNN, Prezi, HuffPost, Adventurer's Codex, Meower, 23andMe, Discovery, Presearch, MIT App Inventor, Bitwarden, Guiding Tech, El Mundo (Spain), Uber, United States Postal Service, Skillshare, Strava, New York Times, iFunny, AirVPN, Stingle Photos, matrix.org\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You can opt out of promotional communications\n",
      "Rating: good\n",
      "Occurrences: 169\n",
      "Found in folders: Amnesty International Limited (UK), Yleisradio, Genius, HideMyAss!, HP | Hewlett-Packard, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), Wolt, LibreOffice, eBay, LBRY, Alzforum, Creative Commons, Profilic, Clockify, Startpage, Symbaloo, CondÃ© Nast, xda-developers, Represent, Gfycat, X, Adobe Services, Amazon, Canva, Attendify, Vivaldi, Kaspersky, CollegeBoard, BitDefender, Weblate, Medium, Encyclopedia Britannica, FastMail, privacypolicies.com, Amazon Prime Video, Firefox Cloud Services, FileFactory, Collins Dictionary, kik-messenger, Unity, Foursquare, Speedtest by Ookla, Disqus, Booking.com, Stack Overflow, BlueMail, UptimeRobot, Brilliant, Quora, Nextcloud, El Mundo (El Salvador), The Filipino Channel, L'Equipe, MuseScore, The Walt Disney Company, Le Figaro, WeMod, Artspace, Simply Earth, Reddit, Pexgle, Tumblr, Netflix, Fiverr, Flipboard, Search Encrypt, Malwarebytes, Washington Post, BBC, Free Software Foundation, Calendly, Le Monde, Vercel, Mega, Alan Thomson Simulation, Pocket, GoDaddy, Twitch, Goodreads, PureVPN, NordVPN, Crain's Chicago Business, Wish, Eltiempo.es, Steam, Coinbase, PayPal, Redbubble, Standard Notes, BBVA, Le Soir, Huawei, ProtonVPN, Moodle, SourceForge, Zoom Video Communications, Gettr, Merriam-Webster, Wattpad, CouchSurfing, Open Collective, Opera, Heaven HR, Headspace, CodeSandbox, OneSignal, SmartNews, Pico, Authy (Twilio), KeyMe, Xiaomi, IKEA, MySudo, Tubi, WordPress.org, Rumble, Fivestars, Open Source Initiative, Bandcamp, Ting Inc., SourceHut, Qwant, Parler, Disney+, Ziff Davis, Healthline, Truth Social, Enpass, MobyGames, WikiHow, ExpressVPN, Kahoot!, Khan Academy, Microspot, Future PLC, ViacomCBS, ISODME, Crunchyroll, Duolingo, BlaBlaCar, Atlassian, Pinterest, FaceApp, Mozilla.org, DeviantArt, Cisco, Swisscows, Prezi, Adventurer's Codex, Hacking with Swift, 23andMe, Discovery, Odysee, Instructure, Guiding Tech, Meredith, United States Postal Service, Riot Games (League of Legends), GeeksforGeeks, Zoho, SpanishDict, Jeuxvideo.com, iFunny, Stingle Photos, XE.com, Dropbox\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: You have the right to leave this service at any time\n",
      "Rating: good\n",
      "Occurrences: 169\n",
      "Found in folders: Tresorit, Apple Services, Audacity, Guilded, HP | Hewlett-Packard, Yello, OpenAI, Remind, OpenStreetMap, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), Wolt, LibreOffice, LinkedIn, LBRY, Clockify, Kongregate, Ubisoft, Open Humans, RiseUp.net, Represent, Replika, Fedora Email, ToS;DR Phoenix, X, Smartmessages, Adobe Services, MeWe, Heroku, FamilyTreeDNA, Vivaldi, Kaspersky, Pearltrees, Snapchat, Medium, YouGov, FastMail, Observable, Firefox Cloud Services, Nslookup, Line Corporation, Instagram, Unity, Foursquare, Stack Overflow, WikiTree, Brilliant, Free Code Camp, Crowdmark, Nextcloud, Consumer News & Business Channel, Yuka, McDonald's, likee, Pixel8Earth, The Walt Disney Company, ASKfm, The Session, Reddit, Pexgle, Tumblr, Netflix, Conjuguemos, Malwarebytes, Jitsi, YouTube, CyberGhost, Wikipedia, Prisma Media, BBC, Follow My Health, Brave, Vercel, Le Monde, Mega, AnonAddy, hCaptcha, edX, Liberapay, GoDaddy, Twitch, Minecraft, PureVPN, Metacafe, NordVPN, Wish, Ableton, ClassDojo, eBuddy, Houseparty, Coinbase, Quizlet, Diario AS, PayPal, Listudy, Redbubble, Standard Notes, Bumble, Sync, Chapril, Zoom Video Communications, Gettr, Wattpad, Google Chrome, Bereal, Opera, Upcloud, Vimeo, Wickr Me, DiscourseHosting, Porton Private Cloudstorage, CodeSandbox, OneSignal, Authy (Twilio), BitChute, Waze, MySudo, Tubi, LightyearVPN, Bandcamp, Ting Inc., V.H. Hess, Parler, Roblox, Disney+, Healthline, Google Analytics, iCloud, Xfinity, Mailfence, Discogs, Google, Instabridge, Privacy Guides, Khan Academy, Patreon, Astronomer, Slack, zmudzinski.me Personal Blog (DEPRECATED), Samsung, Crunchyroll, Brax.Me, Signal, GitHub, pCloud, mastodon.social, Virtual World Computing Cocoon MyData Rewards, HuffPost, Facebook, Meower, Codeberg, Dendreo, 23andMe, Odysee, Presearch, Instructure, Bitwarden, Hulu, SpiderOak, Rofkin, United States Postal Service, Tripadvisor, Patook, SpanishDict, Strava, New York Times, Busuu, Stingle Photos, Dropbox\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: This service gives your personal data to third parties involved in its operation\n",
      "Rating: bad\n",
      "Occurrences: 161\n",
      "Found in folders: Notion, Translate, Guilded, HideMyAss!, HP | Hewlett-Packard, Yello, OpenAI, Remind, Privacy.com, ÐžÐ´Ð½Ð¾ÐºÐ»Ð°ÌÑÑÐ½Ð¸ÐºÐ¸ (Ok.ru), Wolt, LinkedIn, Getty Images, Free, Alzforum, Profilic, Credit Karma, Clockify, Symbaloo, CondÃ© Nast, xda-developers, Represent, Gfycat, Orange, Index Education, MÃ©tÃ©o France, F-List, BFM TV, FamilyTreeDNA, Kaspersky, Pearltrees, IMDb, TikTok, Weblate, YouGov, Encyclopedia Britannica, Gandi, TubeBuddy, Linksys, NBC News, privacypolicies.com, Ouigo, FileFactory, Collins Dictionary, ePlus Technology, Shopify, Line Corporation, Unity, Speedtest by Ookla, Booking.com, UptimeRobot, BlueMail, Brilliant, Free Code Camp, Nextcloud, The Filipino Channel, Microsoft Services, L'Equipe, MuseScore, Wallapop, The Walt Disney Company, Leetify, Le Figaro, Dailymotion, Pexgle, Tumblr, Flatmates, 10minutemail, Flipboard, Conjuguemos, Malwarebytes, Yandex, CyberGhost, Prisma Media, Washington Post, Free Software Foundation, Calendly, Among Us, Le Monde, Element, GoDaddy, Minecraft, PureVPN, NordVPN, Crain's Chicago Business, Wish, Eltiempo.es, ClassDojo, Pantip.com, Coinbase, Veepn, Redbubble, SC Johnson, BBVA, Le Soir, Le Bon Coin, Merriam-Webster, Zoom Video Communications, Wattpad, Google Chrome, Bereal, Opera, Heaven HR, Schoology, OneSignal, Pico, InfinityFree, Authy (Twilio), Vox Media, Selfie2Anime, Amazon AWS, Nvidia, IKEA, Realestate.com.au, ADT, Rumble, BudgetBakers, Open Source Initiative, Movistar, Roblox, Disney+, Ziff Davis, Google Analytics, iCloud, MobyGames, VPN Hamster, Bing, Instabridge, CNED, ExpressVPN, Future PLC, Walmart, ViacomCBS, Alibaba, Astronomer, Samsung, Crunchyroll, BlaBlaCar, Marmiton, FaceApp, Le Parisien, Gather Town, mastodon.social, CNN, Discovery, Odysee, MIT App Inventor, Bitwarden, Hulu, Neurococi, Meredith, Uber, Roadtrip Nation, Rofkin, United States Postal Service, GeeksforGeeks, Tinder, SpanishDict, Jeuxvideo.com, iFunny, matrix.org\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: Blocking first party cookies may limit your ability to use the service\n",
      "Rating: neutral\n",
      "Occurrences: 153\n",
      "Found in folders: Translate, How-To Geek, Guilded, Linguee, HideMyAss!, HP | Hewlett-Packard, Yello, OpenAI, Privacy.com, LibreOffice, Wolt, LinkedIn, Blizzard, LBRY, Free, Creative Commons, Credit Karma, DrugBank, xda-developers, JojoYou (PriEco), Hinterland Games, Gfycat, Orange, Index Education, X, MÃ©tÃ©o France, AliExpress, Toggl Track, Canva, Vivaldi, Kaspersky, Pearltrees, Snapchat, FanFiction, Campus Innovations Kultur, TikTok, BitDefender, Medium, Encyclopedia Britannica, Pure Dating, TubeBuddy, NBC News, privacypolicies.com, Ouigo, Amazon Prime Video, Firefox Cloud Services, Medscape, Avast, ePlus Technology, Bilibili, Shopify, Vouchley, Instagram, WhatsApp, Unity, Foursquare, Speedtest by Ookla, Disqus, Brilliant, El Mundo (El Salvador), Consumer News & Business Channel, The Filipino Channel, L'Equipe, McDonald's, likee, Leetify, Le Figaro, WeMod, Artspace, Reddit, Flatmates, Netflix, 10minutemail, Viagogo, Malwarebytes, Washington Post, Among Us, Le Monde, Mega, Alan Thomson Simulation, hCaptcha, edX, The Movie Database (TMDb), Crain's Chicago Business, Wish, Eltiempo.es, Ableton, Steam, PayPal, Redbubble, SpigotMC, Sync, BBVA, Le Soir, Waterfox, Flickr, Merriam-Webster, Gettr, Fedora, Gap Creek Media, Heaven HR, Headspace, MyAnimeList, Authy (Twilio), Amazon AWS, Arch Linux, Realestate.com.au, Rumble, Tubi, Marca, Fivestars, Qwant, Parler, Roblox, Disney+, Ziff Davis, Healthline, APK Pure, iCloud, Doctolib, VPN Hamster, ExpressVPN, Instabridge, CNED, Khan Academy, OpenHAB, Future PLC, Patreon, Alibaba, Urban Dictionary, Samsung, Telegram, FaceApp, Le Parisien, GitHub, pCloud, Prezi, Facebook, Instructure, Hulu, Neurococi, El Mundo (Spain), United States Postal Service, Swiss Government, Tripadvisor, Riot Games (League of Legends), GeeksforGeeks, Zoho, XE.com, Dropbox\n",
      "--------------------------------------------------------------------------------\n",
      "Clause: The service does not guarantee accuracy or reliability of the information provided\n",
      "Rating: neutral\n",
      "Occurrences: 146\n",
      "Found in folders: Audacity, HP | Hewlett-Packard, OpenStreetMap, Privacy.com, OsmAnd, Wolt, eBay, LBRY, Burger King, Alzforum, The MOD Archive, Credit Karma, Yahoo!, DrugBank, xda-developers, Ubisoft, Hinterland Games, Represent, X, F-List, Canva, FamilyTreeDNA, Vivaldi, Brainly, FanFiction, TikTok, Douban, Pure Dating, Synonymo, Observable, Nslookup, FileFactory, Medscape, Bilibili, Nulled, Shopify, Fandango, WhatsApp, Surfshark, Foursquare, Disqus, Quora, Consumer News & Business Channel, Microsoft Services, The Filipino Channel, El PaÃ­s, McDonald's, likee, ASKfm, WeMod, Artspace, Dailymotion, Reddit, Fiverr, Search Encrypt, YouTube, Washington Post, BBC, Le Monde, Mega, edX, Twitch, Minecraft, Metacafe, NordVPN, Crain's Chicago Business, Wish, Eltiempo.es, Cakey Bot, ClassDojo, Coinbase, Archive of Our Own, BBVA, Huawei, ProtonVPN, National Geographic, SourceForge, Zoom Video Communications, Wattpad, Opera, Wikimedia, TourOfCalifornia.org, Headspace, SpaceHey, MyAnimeList, KidzSearch, BitChute, Canvas, InfinityFree, SparkNotes, KeyMe, Waze, Marca, Rumble, ADT, Tubi, Fivestars, Open Source Initiative, Ryver, Ting Inc., V.H. Hess, Qwant, Return YouTube Dislike, Enpass, MobyGames, Xfinity, t2bot.io, USSeek, ShortcutWorld.com, ExpressVPN, Privacy Guides, Alibaba, Hacker Wars, Crunchyroll, Orchid VPN, BlaBlaCar, Signal, Internet Archive, GitHub, webmail, pCloud, W3Schools, Cisco, Virtual World Computing Cocoon MyData Rewards, BetterHelp, CNN, Bitly, Weatheralex1 Hub, HuffPost, Minds, Discovery, Odysee, Presearch, Bitwarden, Guiding Tech, El Mundo (Spain), United States Postal Service, Swiss Government, Tripadvisor, Patook, Riot Games (League of Legends), Strava, iFunny, Busuu, XE.com\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each (clause, rating) pair\n",
    "pair_counts = Counter((clause_text, rating) for clause_text, rating, _ in clause_pairs)\n",
    "\n",
    "# Get the top 5 most duplicated pairs\n",
    "most_common_pairs = pair_counts.most_common(10)\n",
    "\n",
    "# Map each (clause, rating) to its folder locations\n",
    "pair_locations = defaultdict(set)\n",
    "for clause_text, rating, folder in clause_pairs:\n",
    "    pair_locations[(clause_text, rating)].add(folder)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nðŸ” Top 10 most duplicated clause-rating pairs:\")\n",
    "for (clause_text, rating), count in most_common_pairs:\n",
    "    folders = list(pair_locations[(clause_text, rating)])  # Convert set to list\n",
    "    print(f\"Clause: {clause_text}\\nRating: {rating}\\nOccurrences: {count}\\nFound in folders: {', '.join(folders)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Processing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ WARNING: 'clauses' list is empty in 'RESEARCHCHEMAIAL SWITZERLAND/clauses.json'\n",
      "âš ï¸ WARNING: 'clauses' list is empty in 'Kink.com/clauses.json'\n",
      "\n",
      "âœ… Extracted 14407 clause-rating pairs from the first two companies.\n",
      "\n",
      "('Instead of asking directly, this Service will assume your consent merely from your usage.', 'bad')\n",
      "('This service tracks which web page referred you to it', 'bad')\n",
      "('The service can sell or otherwise transfer your personal data as part of a bankruptcy proceeding or other type of financial transaction.', 'bad')\n",
      "('You must provide your legal name, pseudonyms are not allowed', 'bad')\n",
      "('This service employs third-party cookies, but with opt-out instructions', 'bad')\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data_all_202503120623106\"  # Data directory containing company folders\n",
    "clause_pairs = []\n",
    "\n",
    "# Step 1: Check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"âŒ ERROR: Data directory '{data_dir}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Initialize a counter for the companies\n",
    "company_counter = 0\n",
    "\n",
    "# Step 3: Loop through all company folders inside the data directory\n",
    "for company in os.listdir(data_dir):\n",
    "    company_path = os.path.join(data_dir, company)\n",
    "\n",
    "    # Check if it's a directory (company folder)\n",
    "    if os.path.isdir(company_path):\n",
    "        clause_file = os.path.join(company_path, \"clauses.json\")\n",
    "\n",
    "        # Step 4: Check if clauses.json exists\n",
    "        if not os.path.isfile(clause_file):\n",
    "            print(f\"âŒ ERROR: 'clauses.json' not found in '{company}' folder\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 5: Check if clauses.json is valid JSON\n",
    "            with open(clause_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Step 6: Check if 'clauses' key exists\n",
    "            if \"clauses\" not in data:\n",
    "                print(f\"âš ï¸ WARNING: No 'clauses' key found in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            clauses = data[\"clauses\"]\n",
    "            if not clauses:\n",
    "                print(f\"âš ï¸ WARNING: 'clauses' list is empty in '{company}/clauses.json'\")\n",
    "                continue\n",
    "\n",
    "            # Step 7: Extract (clause_text, rating) pairs\n",
    "            for clause in clauses:\n",
    "                clause_text = clause.get(\"clause_text\", \"\").strip() if clause.get(\"clause_text\") else \"\"\n",
    "                rating = clause.get(\"rating\", \"\").strip() if clause.get(\"rating\") else \"\"\n",
    "\n",
    "                if clause_text and rating:\n",
    "                    clause_pairs.append((clause_text, rating))\n",
    "                else:\n",
    "                    print(f\"âš ï¸ WARNING: Skipping a clause in '{company}' due to missing clause_text or rating.\")\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ ERROR: Invalid JSON in '{company}/clauses.json'\")\n",
    "\n",
    "        # Step 8: Stop after the first 2 companies\n",
    "        company_counter += 1\n",
    "        if company_counter >= 900:\n",
    "            break  # Exit the loop after processing the first two companies\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nâœ… Extracted {len(clause_pairs)} clause-rating pairs from the first two companies.\\n\")\n",
    "for pair in clause_pairs[:5]:  # Print first 5 for checking\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal of duplicates: 14407 clause-rating pairs.\n",
      "âœ… Removed exact duplicates. 1123 unique clause-rating pairs.\n",
      "Number of clauses which appear more than once in our dataset: 447.\n",
      "Number of clauses which we removed because there were already present once: 13284\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize a set to store unique clause-rating pairs, and non-unique clause-rating pais.\n",
    "unique_clause_pairs = set()\n",
    "non_unique_clause_pairs = set()\n",
    "occurrences_of_non_unique_clause_pairs = 0\n",
    "\n",
    "# Step 2: Filter out duplicates by checking if the pair already exists in the set\n",
    "filtered_clause_pairs = []\n",
    "\n",
    "for clause_text, rating in clause_pairs:\n",
    "    # Use a tuple of (description, rating) as the set key\n",
    "    pair = (clause_text, rating)\n",
    "    \n",
    "    # If the pair is not in the set, add it to the filtered list and the set\n",
    "    if pair not in unique_clause_pairs:\n",
    "        filtered_clause_pairs.append(pair)\n",
    "        unique_clause_pairs.add(pair)\n",
    "    else:\n",
    "        non_unique_clause_pairs.add(pair)\n",
    "        occurrences_of_non_unique_clause_pairs +=1\n",
    "\n",
    "unique_clause_pairs = list(unique_clause_pairs)\n",
    "non_unique_clause_pairs = list(non_unique_clause_pairs)\n",
    "\n",
    "# Step 3: Check how many unique pairs there are\n",
    "print(f\"Before removal of duplicates: {len(clause_pairs)} clause-rating pairs.\")\n",
    "print(f\"âœ… Removed exact duplicates. {len(filtered_clause_pairs)} unique clause-rating pairs.\")\n",
    "print(f\"Number of clauses which appear more than once in our dataset: {len(non_unique_clause_pairs)}.\")\n",
    "print(f\"Number of clauses which we removed because there were already present once: {occurrences_of_non_unique_clause_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: If you are the target of a copyright claim, your content may be removed\n",
      "Rating: neutral\n",
      "\n",
      "Title: When the service wants to change its terms, you are notified at least 30 days in advance\n",
      "Rating: good\n",
      "\n",
      "Title: The service will not allow third parties to access your personal information without a legal basis\n",
      "Rating: good\n",
      "\n",
      "Title: The cookies used by this service do not contain information that would personally identify you\n",
      "Rating: good\n",
      "\n",
      "Title: [EU] Information is provided about how they collect personal data\n",
      "Rating: good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of accessing the descriptions and ratings\n",
    "for pair in unique_clause_pairs[:5]:  # Print first 5 pairs for checking\n",
    "    clause_text = pair[0]  # Clause description (x)\n",
    "    rating = pair[1]       # Clause rating (y)\n",
    "    print(f\"clause_text: {clause_text}\\nRating: {rating}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BERT model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Filtering: removing pairs with \"unknown\" as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size: 1074\n",
      "Train+dev set size: 859\n",
      "Test set size: 215\n"
     ]
    }
   ],
   "source": [
    "# Step 3.1: Filter out clauses with 'unknown' ratings\n",
    "filtered_clause_pairs = [(clause_text, rating) for clause_text, rating in unique_clause_pairs if rating != \"unknown\"]\n",
    "\n",
    "# Split the filtered data\n",
    "clauses, ratings = zip(*filtered_clause_pairs)  # Extract clauses and their ratings\n",
    "\n",
    "# Map ratings to integers\n",
    "rating_dict = {\"blocker\": 0, \"bad\": 1, \"neutral\": 2, \"good\": 3}  # Modify if you have different ratings\n",
    "ratings_int = [rating_dict[r] for r in ratings]\n",
    "\n",
    "# Step 3.2: Split data into train, dev, and test sets (80% train, 10% dev, 10% test)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(clauses, ratings_int, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print size\n",
    "print(f\"Data set size: {len(filtered_clause_pairs)}\")\n",
    "print(f\"Train+dev set size: {len(y_temp)}\")\n",
    "print(f\"Test set size: {len(y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Deduplication based on n-gram similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=3):\n",
    "    \"\"\"Convert text into a set of n-grams.\"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in ENGLISH_STOP_WORDS]  # Remove stopwords and non-alphanumeric\n",
    "    return set(ngrams(tokens, n))\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def deduplicate_test_set(train_clauses, test_clauses, test_labels, threshold=0.7, n=3):\n",
    "    \"\"\"Remove test clauses that are too similar to any train clause based on n-gram Jaccard similarity.\"\"\"\n",
    "    train_ngrams = [get_ngrams(clause, n) for clause in train_clauses]\n",
    "    \n",
    "    filtered_test_clauses = []\n",
    "    filtered_test_labels = []\n",
    "    \n",
    "    for test_clause, test_label in zip(test_clauses, test_labels):\n",
    "        test_ngram_set = get_ngrams(test_clause, n)\n",
    "        \n",
    "        # Check similarity with each train clause\n",
    "        max_similarity = max(jaccard_similarity(test_ngram_set, train_set) for train_set in train_ngrams)\n",
    "        \n",
    "        if max_similarity < threshold:\n",
    "            filtered_test_clauses.append(test_clause)\n",
    "            filtered_test_labels.append(test_label)\n",
    "    \n",
    "    return filtered_test_clauses, filtered_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication: test+dev set size is 215\n",
      "After deduplication: test+dev set size is 151\n"
     ]
    }
   ],
   "source": [
    "# Apply deduplication\n",
    "print(f\"Before deduplication: test+dev set size is {len(y_test)}\")\n",
    "X_test, y_test = deduplicate_test_set(X_temp, X_test, y_test, threshold=0.7, n=3)\n",
    "print(f\"After deduplication: test+dev set size is {len(y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Preparing for training LegalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 730\n",
      "Dev set size: 129\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
    "print(f\"Train set size: {len(y_train)}\")\n",
    "print(f\"Dev set size: {len(y_dev)}\")\n",
    "\n",
    "# Step 3.3: Convert into a format that Hugging Face can use\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "dev_data = Dataset.from_dict({\"text\": X_dev, \"label\": y_dev})\n",
    "test_data = Dataset.from_dict({\"text\": X_test, \"label\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8d7b64fd2f4ccc932499b5645d8cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98648f4a8a94d21970566cc1d8f80c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd68a4122340b1b757c067292db6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4455676d7597400fbe3e8acf3da41d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6903c4de63994c0cbb7baa01df972c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b468da0192463993ad97beb70acb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ddc629ba64a1f80a285e746959fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_18591/3550178219.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526352a7ff53465a944fd9b8f921692c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_18591/3550178219.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92dee7487b946939812cf64ad448eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/gjtb3by11p7b_j804bt863qh0000gn/T/ipykernel_18591/3550178219.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 4.2: Define a function to tokenize the input texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples.get('text', \"\"), padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Step 4.3: Apply the tokenizer to the train, dev, and test datasets\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "dev_data = dev_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 4.4: Set the format for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "dev_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "\n",
    "# Step 4.5: Remove the original text filed\n",
    "train_data = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "dev_data = dev_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "test_data = test_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Step 4.6: Make sure we are working with longs\n",
    "train_data = train_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "dev_data = dev_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})\n",
    "test_data = test_data.map(lambda x: {\"label\": torch.tensor(x[\"label\"]).long()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Take a smaller sample (e.g., 5%) of the training data\n",
    "train_sample = train_data.shuffle(seed=42).select(range(int(0.01 * len(train_data))))\n",
    "dev_sample = dev_data.shuffle(seed=42).select(range(int(0.05*len(dev_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)  # Convert logits to predicted labels\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Hyperparameter tuning for Legal-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=3e-05, epochs=3, dropout=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.376700</td>\n",
       "      <td>1.325261</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.095700</td>\n",
       "      <td>1.114964</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.537037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=3e-05, epochs=3, dropout=0.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 02:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.319800</td>\n",
       "      <td>1.123485</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>0.990176</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.751748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=5e-05, epochs=3, dropout=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/54 01:39 < 00:40, 0.37 it/s, Epoch 2.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.312600</td>\n",
       "      <td>1.020145</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.537037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.124300</td>\n",
       "      <td>0.919543</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.647619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch_size=2, lr=5e-05, epochs=3, dropout=0.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/54 00:13 < 01:28, 0.51 it/s, Epoch 0.44/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/6 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: (2, 3e-05, 3, 0.2)\n",
      "Best Weighted F1 Score: 0.7447552447552447\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"batch_size\": [2],  # Try different batch sizes - use [4,8,16,32]\n",
    "    \"learning_rate\": [3e-5, 5e-5],  # Common BERT learning rates - use [2e-5,3e-5,5e-5]\n",
    "    \"num_epochs\": [3],  # Vary number of epochs - use [4,8,12,16]\n",
    "    \"dropout_rate\": [0.1, 0.2]  # Try different dropout rates - use [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Step 2: Track best model\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Step 3: Iterate over all hyperparameter combinations\n",
    "for batch_size, lr, epochs, dropout in product(param_grid[\"batch_size\"], \n",
    "                                               param_grid[\"learning_rate\"], \n",
    "                                               param_grid[\"num_epochs\"], \n",
    "                                               param_grid[\"dropout_rate\"]):\n",
    "    \n",
    "    print(f\"\\nTraining with batch_size={batch_size}, lr={lr}, epochs={epochs}, dropout={dropout}\\n\")\n",
    "\n",
    "    # Step 4: Modify model with dropout\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "    model.config.hidden_dropout_prob = dropout\n",
    "    model.config.attention_probs_dropout_prob = dropout\n",
    "    \n",
    "    # Step 5: Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",  # Don't save all models to save space\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # Step 6: Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_sample,\n",
    "        eval_dataset=dev_sample,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Step 7: Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Step 8: Get weighted F1-score\n",
    "    f1 = metrics.get(\"eval_f1\", 0)\n",
    "\n",
    "    # Step 9: Track best model\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = (batch_size, lr, epochs, dropout)\n",
    "        best_model = model  # Save the best model in memory\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Weighted F1 Score:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Evaluate the best model on the test set\n",
    "if best_model is not None:\n",
    "    print(\"\\nEvaluating the best model on the test set...\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_data,  # Use the test dataset\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    test_metrics = trainer.evaluate()\n",
    "    print(\"\\nTest Set Metrics:\", test_metrics)\n",
    "else:\n",
    "    print(\"No best model found. Check hyperparameter tuning process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the train set\n",
    "train_results = trainer.evaluate(train_data)\n",
    "# Print loss and accuracy\n",
    "print(f\"Train Loss: {train_results['eval_loss']:.4f}\")\n",
    "print(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")  # Accuracy from compute_metrics\n",
    "# Evaluate on the dev set\n",
    "dev_results = trainer.evaluate(dev_data)\n",
    "# Print loss and accuracy\n",
    "print(f\"Dev Loss: {dev_results['eval_loss']:.4f}\")\n",
    "print(f\"Dev Accuracy: {dev_results['eval_accuracy']:.4f}\")  # Accuracy from compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_data)\n",
    "\n",
    "# Print loss and accuracy\n",
    "print(f\"Test Loss: {test_results['eval_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")  # Accuracy from compute_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
